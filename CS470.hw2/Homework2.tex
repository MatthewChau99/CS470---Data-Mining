\usepackage{multirow}
\documentclass[12pt]{article}
\usepackage{design_ASC}

\setlength\parindent{0pt} %% Do not touch this

%% -----------------------------
%% TITLE
%% -----------------------------
\title{CS470 Homework 2} %% Assignment Title

\author{Matthew Chau\\ %% Student name
}

\date{\today} %% Change "\today" by another date manually
%% -----------------------------
%% -----------------------------

%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\setlength{\droptitle}{-5em}
%% %%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

% --------------------------
% Start here
% --------------------------

% %%%%%%%%%%%%%%%%%%%
\section*{Collaboration Statement}
For this assignment, I have not consulted or asked for help from anyone for the completion of this assignment.
% %%%%%%%%%%%%%%%%%%%


% %%%%%%%%%%%%%%%%%%%
\section*{Runtime Data}
% %%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%
\subsection*{T10I4D100K.txt}
% %%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{Dataset}                 & \multicolumn{6}{c|}{\textit{T10I4D100K.txt}}     \\ \hline
\textbf{Minimum Support}         & 500   & 750     & 1000 & 1250   & 1500  & 1750   \\ \hline
\textbf{Total Lines of Data}     & \multicolumn{6}{c|}{100000}                      \\ \hline
\textbf{Ratio}                   & 0.005 & 0.0075  & 0.01 & 0.0125 & 0.015 & 0.0175 \\ \hline
\textbf{Runtime (s)}             & 12.26 & 0473852 & 6.18 & 5.72   & 3.53  & 3.14   \\ \hline
\textbf{Number of Frequent Sets} & 1073  & 561     & 385  & 310    & 237   & 186    \\ \hline
\end{tabular}
\end{table}

% %%%%%%%%%%%%%%%%%%%
\subsection*{retail.txt}
% %%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{Dataset}                 & \multicolumn{6}{c|}{\textit{retail.txt}}            \\ \hline
\textbf{Minimum Support}         & 500    & 750    & 1000   & 1250   & 1500   & 1750   \\ \hline
\textbf{Total Lines of Data}     & \multicolumn{6}{c|}{88162}                          \\ \hline
\textbf{Ratio}                   & 0.0057 & 0.0085 & 0.0113 & 0.0142 & 0.0170 & 0.0198 \\ \hline
\textbf{Runtime (s)}             & 5.65   & 4.12   & 3.41   & 2.94   & 2.72   & 2.56   \\ \hline
\textbf{Number of Frequent Sets} & 468    & 219    & 135    & 91     & 68     & 56     \\ \hline
\end{tabular}
\end{table}

\begin{flushleft}
Above are the data of the execution of the Apriori Algorithm on two different datasets: T10I4D100K.txt, which consists of 100,000 different transactions, and retail.txt, which consists of 88162 different transactions. The minimum support are hold the same to compare the different runtime.
\end{flushleft}


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{Graph1.png}
    \caption{Figure 1}
    \label{Min Sup Ratio vs Runtime}
\end{figure}

\begin{flushleft}
From Figure 1 below, we notice that both curves have downward slope with inward curves, and the execution time for retail.txt is much shorter than T10I4D100K.txt, around half of the runtime when the minsup ratio is around 0.005. This also indicates that the runtime tend to have a positive relationship with the number of transactions in a file.
\end{flushleft}

\section*{Optimization Methods}

\begin{flushleft}
When designing this algorithm, I have used several optimizing methods to boost the running speed. There are two data containers: subset\textunderscore mapping, a dictionary that maps rows from the data set to the set of all length-k candidate frequent subsets; freq\textunderscore count, a dictionary that maps each frequent set to its number of occurrence in the data set.
\end{flushleft}
\begin{flushleft}
First of all, the data set is entered into subset\textunderscore mapping, a dict in python with average cost of O(1) in query, insert, and remove. The benefit of using a dynamic data set is to delete redundant rows that cannot generate new candidates, so that the iteration of the data set would be much quicker as the size of the data decreases.
\end{flushleft}
\begin{flushleft}
Secondly, I use a temporary freq\textunderscore count\textunderscore local to store a set of subsets of length k at kth iteration to avoid iterating all subsets from length 1 to k. Then the frequency count in freq\textunderscore count\textunderscore local is added to the global freq\textunderscore count. This reduces O(size of dataset) operation to O(number of length k frequent subsets), which makes it more efficient.
\end{flushleft}

\section*{Experience and lesson learned}
While doing this assignment, I have realized how efficiency of algorithms is, and have gained much experience on manipulating with large data sets. At first, my algorithm runs tremendously slow - around 35 minutes with incorrect results. After pruning and some optimization, it is reduced to around 18 minutes; and later on 5 minutes. With more and more optimization, as lots of loops being deleted, redundant data being removed, better data structures being used, it's finally maintained at around 11 seconds, and I believe it can still be further optimized.
\\\\
Also, when debugging for mistakes, I have learned to divide the problem into small sub-problems. For example, I have reduced the problem to 1. Generate length 1 candidate subsets, 2. Generate length 2 candidate subsets, and so on. After generating all candidates, I generalized and added a while loop that loops until no more candidates can be generated.
\\\\
Making the data set smaller is also a good way to check the accuracy of the algorithm in a much shorter time. To debug, I have manually added a couple rows of data so that I could do human calculations on the data set, while comparing to the results of the algorithm. Printing between different sections is also a good way to find out where the error is located exactly. It helped me find a lot of errors in my code, and gave me a clear idea of what the variables look like at a particular moment.



\end{document}
